from bs4 import BeautifulSoup
"""
Beautiful Soup — это библиотека Python для извлечения данных из файлов HTML и XML и является одним из
основных инструментов для парсинга сайтов. Библиотека Beautiful Soup - швейцарский нож при написании парсеров!
С помощью нее мы можем разбирать на винтики полученую нами html страничку (её DOM-дерево) и обращаясь к html-тегам
и их атрибутам забирать нужную нам информацию.
"""

# для наглядности покажу как можно посмотреть DOM-дерево любого сайта и что оно из себя представляет

"""
Для того чтобы установить библиотеку Beautiful Soup нам нужно перейти на сайт PyPi 
https://pypi.org/
"""

# сейчас наглядно покажу

"""
Для работы с данной библиотекой нам сначала нужно её импортировать! Делается это следующим образом:
from bs4 import BeautifulSoup
"""

"""
Сначала, для того чтобы начать работать, нам нужно открыть файл нашей страницы!
В Python есть встроенная функция open(). С ее помощью можно открыть любой файл на компьютере. 
Технически Python создает на его основе объект. Кроме имени открываемого файла, функция open() имеет ещё и 
access_mode - режим открытия файла (чтение, запись, чтение и запись, добавление нового содержимого).
После открытия файла в Python его нужно закрыть. Таким образом освобождаются ресурсы и убирается мусор.
Для этого применяется функция close(). НО! Есть инструкция with, которая упрощает обработку исключений с помощью 
инкапсуляции (один из основных принципов ООП, позже будем изучать) начальных операций, а также задач по закрытию и 
очистке. В таком случае инструкция close не нужна, потому что with автоматически закроет файл.
"""

# это только потому, что у нас код находится в файле на компьютере!
# для работы с url используется другая библиотека requests
with open("html\index.html", encoding="utf-8") as file:  # file - переменная указатель, через которую мы будем ссылаться на наш файл
    src = file.read()

# print(src) # для наглядности
# print(type(src))

# для корректного отображения русских букв можно вторым параметром передать encoding="utf-8"

"""
Теперь, если говорить простым языком, нам нужно скормить код нашей страницы библиотеке Beautiful Soup, для того чтобы 
мы могли пользоваться её методами для извлечения информации! Это тоже самое, как если бы мы сделали приведение типа 
данных Integer к типу данных String, для того чтобы воспользоваться методами, которые есть у String, но не у Integer 
(или строку текста к списку). Это выглядит следующим образом:
"""

soup = BeautifulSoup(src, "lxml")
# print(type(soup))
# print(soup)

"""
Функция BeautifulSoup() принимает 2 параметра. Первый  - код страницы (в нашем случае мы код страницы записали 
в переменную src). Второй - парсер!
"""

# рассказать по парсеру

"""
У библиотеки Beautiful Soup есть много различных методов. Например у каждой страницы в интернете есть тег title. 
Получить информацию из этого тега ты можем при помощи метода title
"""

title = soup.title
# print(title)

"""
Таким образом, применяя те или иные методы к нашему объекту, мы получаем блоки кода, содержащие информацию 
не только о самом значении, но так же получаем и их теги, классы, id, ссылки и т.д.
Для того, чтобы получить только сами значения, можно применить методы text, get_text() или string (считается устаревшим) 
"""

# title_value = title.get_text()
# print(title_value)

"""
Одними из основных методов библиотеки Beautiful Soup являются методы find() и find_all(). Работа именно с этими 
методами будет решать 90% ваших задач. Эти методы работают на странице сверху вниз. При использовании метода find()
нам вернется ПЕРВЫЙ найденый объект с параметрами, переданными в метод find(). А при использовании метода find_all()
нам вернутся ВСЕ объекты В ВИДЕ СПИСКА с параметрами, переданными методу find_all()
"""

# наглядно покажу работу методов на этом примере
# post = soup.find("div", class_="post__title")
# print(post)
# print()
# all_posts = soup.find_all("div", class_="post__title")
# print(all_posts)

"""
Мы так же можем использовать связку методов, словно шагая вглубь. Это достаточно полезный функционал.
Например в нашем файле много тегов <span>, и если мы хотим получить конкретное значение, содержащееся под тегом <span>,
нам нужно будет использовать связку методов, т.к. метод find() вернет только первый найденый объект, а метод find_all()
вернет их нам все, после чего придется ещё делать дополнительные операции со списком! вот пример как можно получить 
имя пользователя находящееся в теге <span>
"""

# user_name = soup.find("div", class_="user__name").find("span").text
# print(user_name)

"""
На нашей страничке также имеются ссылки на соц.сети. Давайте их тоже получим =)
"""

# links = soup.find("div", class_="social__networks").find("ul").find_all("a")
# print(links)

# for link in links:
#     print(link.text)

"""
Если применяем метод text, то ссылки пропадают вместе с тегами. А это не совсем то что нам хочется =) 
Таким образом мы получили только значения, содержащиеся в теге <a>. Оно и верно! Ведь сама ссылка находится в 
атрибуте href (ссылки ВСЕГДА хранятся в атрибуте с названием href). И для того чтобы получить наши ссылки, нам 
необходимо воспользоваться методом get(), в который в качестве параметра передадим название атрибута href.
С помощью метода get() мы можем забирать ЛЮБОЙ атрибут, а не только href.
"""

# for link in links:
#     link_url = link.get("href")
#     print(f"{link.text}: {link_url}")

"""
Для перемещения по DOM-дереву так же имеется несколько полезных методов. Например методы find_parent() и find_parents().
Как можно догадаться по названиям, они ищут родителя или родителей элементов, т.е. их действия аналогичны методам find()
и find_all() с той лишь разницей, что они поднимаются по коду вверх, а не опускаются вниз. Для примера, в нашей странице 
присутствует блок кода под названием Блог, в котором есть несколько статей, имеющих собственную структуру.
Возьмем за отправную точку div с классом post__text и посмотрим как это работает
"""

# post_div = soup.find("div", class_="post__text").find_parent()
# print(post_div)

"""
Таким образом мы находим блок кода первого родителя, которым является div с классом user__post__info.
В метод find_parent() мы так же можем передавать параметры поиска, таким образом мы можем находить не просто первого 
попавшегося родителя, а первого попавшегося родителя с заданными параметрами. Например для того чтобы найти второго 
родителя, дазадим парамерты методу find_parent()
"""

# post_div = soup.find("div", class_="post__text").find_parent("div", class_="user__post")
# print(post_div)

"""
А метод find_parents() работает так, что поднимается вверх и возвращает и тег body и даже сам html тег
"""

# post_divs = soup.find("div", class_="post__text").find_parents()
# print(post_divs)

"""
Мы можем так же ему ставить фильтры, или ограничения на поиск.
"""

# post_divs = soup.find("div", class_="post__text").find_parents("div", "user__post")
# print(post_divs)

"""
Следующими полезными методами для перемещения по коду являются методы next_element и previous_element.
Из названия так же можно догадаться что нас ждёт. next_element - двигается пошагово и возвращает следующий элемент в 
коде. previous_element - предыдущий. Сейчас мы посмотрим на поведение метода next_element. Допустим нам нужно получить 
название поста с тегом h3. Для этого обращаемся к div-у с классом post__title и вызываем метод next_element
"""

# next_el = soup.find("div", class_="post__title").next_element
# print(next_el)

"""
На выходе мы получили пустоту =) Казалось бы, мы должны были получить заголовок статьи! Но почему так произошло?
Дело в том, что метод next_element двигается по коду весьма дотошно выполняя свою работу. И на самом деле он вернул 
нам перенос строки (\n). и для того чтобы продвинуться дальше, нам нужно вызвать метод повторно.
"""

# next_el = soup.find("div", class_="post__title").next_element.next_element
# print(next_el)

"""
Есть ещё похожий метод find_next(), который сразу вернет следующий элемент
"""

# next_el = soup.find("div", class_="post__title").find_next()
# print(next_el)

"""
Метод previous_element работает с точностью наоборот, двигаясь по дереву снизу вверх.
Следующие интересные методы это find_next_sibling() и find_previous_sibling(), которые ищут и возвращают следуюшие и 
предыдущие элементы внутри искомого тега. (т.е. простыми словами методы ищут братьев и сестер. Если после нашего тега
больше нет братьев и сестер, то для метода find_next_sibling() вернется None. Такая же ситуация и с методом 
find_previous_sibling())
"""

# next_sib = soup.find("div", class_="post__title").find_next_sibling()
# print(next_sib)

# previous_sib = soup.find("div", class_="post__title").find_previous_sibling()
# print(previous_sib)

"""
Все эти методы можно комбинировать между собой, создавая своеобразные цепочки из методов. Комбинации методов дают нам 
просто неограниченные возможности для парсинга и перемещения внутри html кода. Все методы имеют и множественное число,
и отличаются они лишь тем, что получаем мы либо 1 объект, либо множество объектов в списке.
"""